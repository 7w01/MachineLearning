# 特征选择与稀疏学习



## 特征选择方法



特征选择是重要的**数据预处理**过程，目的是保留重要特征，去掉冗余特征



特征选择方法包括**特征子集搜索机制**和**特征子集评价机制**



### 特征子集搜索基本方法



若没有先验知识，则只能遍历所有可能的特征子集，但是很可能会遇到组合爆炸

基于**贪心策略**，可利用**前向搜索**、**后向搜索**、**双向搜索**



#### 前向搜索

将每个特征看作一个候选子集，对$d$个单特征子集进行评价，假定{$a_2$}最优，则将{$a_2$}作为第一轮的选定集

然后在上一轮选定集中加入一个特征，构成两个特征的候选子集，假定{$a_2,a_4$}最优，则将${a_2,a_4}$作为第二轮的选定集

若在第k+1轮时，得到的选定集的评价不如第k轮的选定集，则停止生成新的选定集，并将第k轮的选定集作为特征选择结果



#### 后向搜索

从完整的特征集合开始，逐渐减少特征，每次尝试去掉一个特征，过程与前向搜索类似



#### 双向搜索

将前向搜索与后向搜索相结合，在每一轮中逐渐增加特征(这些特征在后续中确定不会被去除)，同时减少无关特征



### 特征子集评价基本方法



能**判断**对数据集$D$**不同划分**的**差异**的机制都能用于特征子集的评价



### 过滤式选择(filter)



**先**对数据集进行特征选择，**然后**再训练学习器，特征选择过程与后续学习器无关



#### Relief

Relief是著名的过滤式特征选择方法，给方法设计了一个**相关统计量**来度量特征的重要性

该统计量是一个向量，每个分量对应一个初始特征，而特征子集的重要性由每个特征对应的相关统计量的分量决定，特征选择方式有两种

​	1.选择一个**阈值**$t$，然后选择比$t$大的相关统计量的分量所对应的特征

​	2.指定欲选择的**特征数量**$k$，然后选择前$k$大个相关统计量的分量所对应的特征



算法**关键**在于确定相关统计量，相关统计量对应特征$j$的分量为
$$
\delta^j=\sum_i(-diff(x^j_i,x^j_{i,nh})^2+diff(x^j_i,x^j_{i,nm})^2)
\tag{1}
$$
其中，$x^j_i$表示样本$x_i$在$j$上的分量，$x_{i,nh}$表示与$x_i$同类别且距离最近的样本(near-hit)，$x_{i,nm}$表示与$x_i$异类别且距离最近的样本(near-miss)

在计算相关统计量时，只需对数据集$D$采样后计算，所以Relief的时间开销随采样次数和原始特征数**线性**增加，因此是**高效**的



#### Relief-F

是Relief算法在**多分类**任务中的扩展变体
$$
\delta^j=\sum_i[-diff(x^j_i,x^j_{i,nh})^2+\sum_{l\neq k}(p_l\times diff(x^j_i,x^j_{i,l,nm})^2)]
\tag{2}
$$
其中，$p_l$为第$l$类样本在数据集$D$中所占的比例，$x_{i,l,nm}$表示类别为$l$且距离$x_i$最近的样本



### 包裹式选择(wrapper)



把最终将要使用的**学习器的性能**作为特征子集的评价准则，故从最终的学习器**性能**来看，比过滤式特征选择**更好**，但是时间**开销大**



#### LVW(Las Vegas Wrapper)

在拉斯维加斯方法(Las Vegas method)框架下使用**随机策略**来进行子集搜索，并以最终分类器的误差为特征子集评价准则



斯维加斯方法类似蒙特卡洛方法，区别是：若有时间限制，则拉斯维加斯方法或者给出满足要求的解，或者不给出解；蒙特卡洛方法一定会给出解，但是解不一定满足要求；若无时间限制，则两者相同，都能给出满足要求的解



### 嵌入式选择(embedding)



将特征选择与学习器训练过程融为一体，即在学习器训练过程中自动地进行了特征选择



考虑最简单的线性回归模型，以平方误差为损失函数，则优化目标为
$$
\mathop{min}_{\omega}\sum^m_{i=1}(y_i-\omega^Tx_i)^2
\tag{3}
$$
当样本特征多，样本数量少时，很容易陷入过拟合，可以引入**正则化**项，若使用$L_2$范数正则化，则有
$$
\mathop{min}_{\omega}\sum^m_{i=1}(y_i-\omega^Tx_i)^2+\lambda||\omega||^2_2
\tag{4}
$$
其中，正则化参数$\lambda>0$，(4)式称为**岭回归**

若使用$L_1$范数正则化，则有
$$
\mathop{min}_{\omega}\sum^m_{i=1}(y_i-\omega^Tx_i)^2+\lambda||\omega||_1
\tag{5}
$$
其中，正则化参数$\lambda>0$，(5)式称为**LASSO**



$L_1$范数和$L_2$范数正则化都有助于降低过拟合风险，但前者还会带来一个好处：他比后者更易于获得**稀疏解**，即它求得的$\omega$会有更少的非零分量



## 稀疏学习



不妨把数据集$D$考虑成一个矩阵，矩阵中存在很多零元素，但零元素不是以整行、整列的形式存在的，即样本具有**稀疏表达形式**

具有这样表达形式的数据对学习任务来说有不少好处，例如线性支持向量机，由于数据具有稀疏性，使得大多数问题变得**线性可分**

同时稀疏样本不会造成**储存**上的巨大负担，稀疏矩阵已有很多高效的储存方法



### 字典学习



将数据**转化为稀疏**表示形式，使之享有稀疏性的好处；我们可以学习出一个**字典**，将将数据转化为稀疏表示形式



对给定数据集{$x_1,x_2,...,x_m$​}
$$
\mathop{min}_{B,\alpha_i}\sum^m_{i=1}||x_i-B\alpha_i||^2_2+\lambda\sum^m_{i=1}||\alpha_i||_1
\tag{6}
$$
其中，$B\in R^{d\times k}$为字典矩阵，$k$称为字典的词汇量，通常由用户指定，$\alpha_i\in R^k$是样本$x_i$的稀疏表示



### 压缩感知



在现实任务中，为了便于传递、储存，通常对数字信号进行压缩，这有可能损失一些信息；压缩感知即通过部分信息来恢复全部信息


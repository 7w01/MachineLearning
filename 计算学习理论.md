# 计算学习理论



## 常用不等式



### Jensen不等式

对任意凸函数$f(x)$，有
$$
f(E(x))\leq E(f(x))
\tag{1}
$$


### Hoeffding不等式

若$x_1,x_2,...,x_m$为m个独立随机变量，且满足$0\leq x_i\leq 1$，则对任意$\epsilon >0$，有
$$
P(\frac{1}{m}\sum^m_{i=1}x_i-\frac{1}{m}\sum^m_{i=1}E(x_i)\geq \epsilon)\leq exp(-2m\epsilon^2)\\
P(|\frac{1}{m}\sum^m_{i=1}x_i-\frac{1}{m}\sum^m_{i=1}E(x_i)|\geq \epsilon)\leq 2exp(-2m\epsilon^2)
\tag{2}
$$


### McDiarmid不等式

若$x_1,x_2,...,x_m$为m个独立随机变量，且对任意$1\leq i\leq m$，函数$f$满足



## PAC学习



$c$表示“概念”，代表映射$X\rightarrow Y$，它决定实例$x$的真实标记$y$，若对任意样例有$c(x)=y$，则称$c$为**目标概念**，所有目标概念构成的集合称为**概念类**，用符合$C$ 表示



给定学习算法$L$，它所考虑的所有可能概念的集合称为**假设空间**，用符号$H$表示. 若$c\in H$，则称学习算法$L$是**可分的**，亦称**一致的**



对于给定训练集$D$，我们希望基于学习算法$L$学得的模型所对应的假设$h\in H$**尽可能**接近目标概念$c$，因为要是希望精确地学到目标概念$c$​会产生过拟合



### PAC辨识

对$0<\epsilon,\delta<1$，所有$c\in C$和分布$D$，若存在学习算法$L$，其输出假设$h\in H$满足
$$
P(E(h)\leq \epsilon)\geq1-\delta
\tag{4}
$$
其中，$E(h)$为$h$的泛化误差，则称学习算法$L$能从假设空间$H$中PAC辨识概念类$C$，即以$1-\delta$的**概率**学得目标概念$c$​的**近似**



### PAC可学习

令$m$表示从分布$D$中独立同分布采样得到的样例数目，$0<\epsilon,\delta<1$，对所有分布$D$，若存在学习算法$L$和多项式函数$poly(.,.,.,.)$，使得对于任何$m\geq ploy(1/\epsilon,1/\delta,size(x),size(c))$，$L$能从假设空间$H$中PAC辨识概念类$C$，则称概念类$C$



### PAC学习算法

若学习算法$L$使概念类$C$为PAC可学习的，且$L$的运行时间也是多项式函数$poly(1/\epsilon,1/\delta,size(x),size(c))$，则称概念类$C$是高效PAC可学习的，称$L$为概念类$C$的PAC学习算法



### 样本复杂度

满足PAC学习算法$L$所需的$m\geq ploy(1/\epsilon,1/\delta,size(x),size(c))$中最小的$m$，称为学习算法$L$的样本复杂度

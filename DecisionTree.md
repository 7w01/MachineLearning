# 决策树(Decision Tree)



## 输入： 

训练集 D={($x_1$,$y_1$), ($x_2$,$y_2$),...,($x_m$,$y_m$)};	属性集 A={$a_1$,$a_2$,$a_3$,...,$a_d$};



## 过程：

***TreeGenerate(D, A):***

​	**if** D中样本全属于同一类别C:

​		将node标记为C类叶结点;

​		return;

​	**if** A = $\varnothing$​ or D 中样本在A上取值相同:

​		将node标记位叶结点，其类别标记为D中样本数最多的类;

​		return;

​	**else if**

​		从A中选择最优分划属性$a_*$​;

​		**for** $a_*$的每一个值 $a_*^v$​ do:

​			为node生成一个分支，令$D_v$表示D中在$a_*$上取值为$a_*^v$的样本子集

​			**if** $D_v$=$\varnothing$:

​				将分支结点标记为叶结点，其类别标记为D中样本最多的类;

​				return;

​			**else**

​				以**TreeGenerate($D_v$, A\\{$a_*$})**为分支结点;



**算法关键在于选择怎样的策略去选择划分属性，不同算法基于不同的选择方式**



## 指标：



#### 信息熵：

C为类别总数，$p_k$表示在样本集合D中第k类样本所占的比例为$p_k$，则D的信息熵定义为：
$$
Ent(D)=-\sum^{C}_{k=1}p_klog_2p_k
$$
假定离散属性a有V个可能的取值，若使用a来对样本集D进行划分，则会产生V个分支结点，其中第v个分支结点包含了在属性a上取值为$a^v$的样本，记为$D^v$. 我们可以算出$D^v$的信息熵*Ent*($D^v$)，再考虑不同分支结点的样本数不同，样本数多的分支影响大，故引出信息增益：



#### 信息增益：

$$
Gain(D,a)=Ent(D)-\sum_{v=1}^{V}\frac{D_v}{D}Ent(D_v)
$$

一般而言，**信息增益越大**，意味着使用属性a进行划分所获得的**纯度更大**. 这种方法可能对取值数目较多的属性有所偏好，故不直接使用信息增益，而是使用增益率来选择属性：



#### 增益率：

$$
Gain\_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}
$$

其中
$$
IV(a)=-\sum^{V}_{v=1}\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}
$$
称为a的**固有值**. 属性a的可能取值数目V越大，则*IV*(a)的值通常会越大



#### 基尼指数：

##### **基尼值：**

$$
Gini(D)=\sum^{C}_{k=1}\sum_{k'\neq k}p_kp_{k'}\\
=1-\sum^{C}_{k=1}p^2_k
$$
直观上，*Gini(D)*反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率. 因此*Gini(D)*越小，数据集D的纯度越高.

##### **基尼系数：**

$$
Gini\_index(D,a)=\sum^{V}_{v=1}\frac{|D^v|}{|D|}Gini(D^v)
$$
于是在选择分划后基尼指数最小的属性.



##  过拟合处理：



#### 预剪枝：

在决策时的生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能的提升，则停止划分并将当前结点标记为样本数目最多类型的叶结点. 显著节省了开销，但是带来了欠拟合的风险.



#### 后剪枝：

先从训练集中生成一棵完整的决策树，然后自底向上地对非叶子结点进行考察. 欠拟合风险小，泛化性能往往优于预剪枝，但是开销大.



## 连续值处理：



此处采用**连续属性离散化技术**中的**二分法**，将样本中连续属性a的取值从小到大排序，记为{$a^1$,$a^2$,...，$a^n$}，取每相邻两个值的中点作为分划点，则划分点集合为
$$
T_a=\{\frac{a^i+a^{i+1}}{2}|1\leq i\leq n-1\}
$$
**信息增益改写**为：
$$
Gain(D,a)=\max_{t\in T_a}Gain(D,a,t)
$$

## 缺失值处理：



假定为每个样本赋予一个权重$w_x$，令$\tilde{D}$表示D中在属性a上没有缺失值的样本子集.

 定义：
$$
\rho =\frac{\sum_{x\in \tilde{D}}w_x}{\sum_{x\in D}w_x}\\
\tilde{r}_v=\frac{\sum_{x\in \tilde{D}^v}w_x}{\sum_{x\in \tilde{D}}w_x}
$$
**信息增益改写**为：
$$
Gain(D,a)=\rho \times Gain(\tilde{D},a)\\
=\rho \times (Ent(\tilde{D})-\sum_{v=1}^{V}\tilde{r}_vEnt(\tilde{D}^v))
$$
根据改写后的信息增益选择最优的分划属性a. 若样本x在a上的取值已知，则将x划入与其取值对应的子结点，且权重保持不变；若样本在a上的取值未知，则将x划入所有子结点，并将**权重调整**为$\tilde{r}_v \cdot w_x$. 直观来看，就是让未知取值的样本以不同频率作为不同概率划入到不同的子结点中去.
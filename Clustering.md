# 聚类(Clustering)

聚类算法是”**无监督学习**“中研究最多、应用最广的算法. 聚类尝试将数据集中的样本划分为若干个不相交的子集，每个子集称为一个“**簇**”.

## 性能度量

### 外部指标

将聚类结果与参考模型进行比较



### 内部指标

直接考察聚类结果，通常涉及**距离计算**. 对于有序属性，通常采用闵可夫斯基距离；对于无序属性，可采用VDM方法：
$$
VDM_p(a,b)=\sum^k_{i=1}|\frac{m_{u,a,i}}{m_u,a}-\frac{m_{u,b,i}}{m_u,b}|^p
$$
$m_{u,a,i}$表示在第i个样本簇中在属性u上取值为a的样本数



## 原型聚类

### k均值(k-means)

采用贪心策略，通过迭代优化来近似求解最小化误差：
$$
E=\sum^{k}_{i=1}\sum_{x\in C_i}||x-u_i||^2_2
$$
**输入：**无标签样本集D={$x_1,x_2,...,x_m$}，聚类簇数k

**过程：**

​	从D中随机选择k个样本作为初始均值向量{$u_1,u_2,...,u_k$}

​	**repeat**

​		$C_i=\empty(i\le i\le k)$

​		**for** i=1,2,...,m

​			计算样本$x_i$与各均值向量$u_j(1\le j\le k)$的距离：$d_{ij}=||x_i-u_j||_2$

​			将$x_i$标记为距离最近的均值向量$u_j$，并将$x_i$划入簇$C_j$​

​		**for** i=1,2,...,k

​			计算新均值向量：$u^`_i=\frac{1}{|C_i|}\sum_{x\in C_i}x$

​	**break** **until** 均值向量不变



### LVQ(Learning Vector Quantization)

数据集带有类别标记，利用样本监督信息辅助聚类



**输入：**标签样本集D={$(x_1,y_1),(x_2,y_2),...,(x_m,y_m)$

​	标签类别数为k

​	学习率$\eta\in (0,1)$

**过程：**

​	从D中随机选择k个不同标签的样本作为初始均值向量{$u_1,u_2,...,u_k$}

​	**repeat**

​		从样本集D中随机选取样本$x_i$

​		计算样本$x_i$与各均值向量$u_j(1\le j\le k)$的距离：$d_{ij}=||x_i-u_j||_2$

​		找出距离最近的$u_j$

​		**if** $y_i$ = j

​			$u_j=u_j+\eta(x_i-u_j)$

​		**else**

​			  $u_j=u_j-\eta(x_i-u_j)$​

​		**break until** 满足停止条件：向量变化小 或 到达最大迭代次数



## 密度聚类

### DBSCAN

定义

**核心对象：**若$x_i$的$\epsilon$-邻域内至少包含MinPts个样本，则$x_i$​是一个核心对象

**密度直达：**若$x_j$位于$x_i$的$\epsilon$-邻域内，且$x_i$是核心对象，则$x_j$由$x_i$密度直达

**密度可达：**密度直达序列

**密度相连：**若存在$x_k$使得$x_i$和$x_j$均由$x_k$密度可达，则$x_i$和$x_j$密度相连



**输入：**无标签样本集D={$x_1,x_2,...,x_m$}

​	邻域参数($\epsilon$, MinPts)

**过程：**

​	初始化核心对象集合$\Omega=\empty$

​	将所有为核心对象的样本加入$\Omega$中

​	随机选一个核心对象o$\in \Omega$，由此出发将所有**密度可达**且未访问的样本加入该簇中



## 层次聚类

### AGNES

采用**自底向上**策略，在每一步找出距离最近的两个簇进行合并，不断重复，直到达到预设簇数目

关键在于如何计算簇之间的距离：

​	1.最小距离（单链接算法）

​	2.最大距离（全链接算法）

​	3.平均距离（均链接算法）

​			


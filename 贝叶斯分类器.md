# 贝叶斯分类器(Bayesian desicion theory)



将样本$x$分类为$c_i$所产生的期望损失称为**条件风险**
$$
R(c_i|x)=\sum^N_{j=1}\lambda_{ij}P(c_j|x)
\tag{1}
$$
其中，$\lambda_{ij}$是真实标记$c_j$误分类为$c_i$所产生的损失

我们的任务是寻找一个判断准则$h(x)$，以最小化**总体风险**
$$
R(h)=E_x[R(h(x)|x)]
\tag{2}
$$
对每个样本，$h(x)$ 能最小化条件风险，则总体风险也将被最小化，即**贝叶斯判定准则**



## 贝叶斯最优分类器


$$
h^*(x)=\mathop{argmin}_{c\in Y}R(c|x)
\tag{3}
$$
与之对应的$R(h^*)$称为**贝叶斯风险**，反应了通过机器学习所能产生的**模型精度**的**理论上限**

若$\lambda_{ij}=\begin{cases}0, & i=j;\\ 1, & i\neq j.\end{cases}$	即**分类错误率**，则贝叶斯最优分类器为
$$
R(c|x)=1-P(c|x)\\
R(h)=\mathop{argmax}_{c\in Y}P(c|x)
\tag{4}
$$
不难看出，想要使用贝叶斯判定准则来最小化决策风险，首先要获得**后验概率**$P(c|x)$



## 获得后验概率



获得后验概率主要有两种策略：



### 判别式模型

给定$x$，直接建模$P(c|x)$来预测$c$

ex：**决策树、BP神经网络、支持向量机**等



### 生成式模型

先联合概率分布$P(x,c)$，然后由此获得$P(c|x)$



## 生成式模型



对于生成式模型，必然考虑
$$
P(c|x)=\frac{P(c,x)}{P(x)}
\tag{5}
$$
基于**贝叶斯定理**
$$
P(c|x)=\frac{P(x|c)P(c)}{P(x)}
\tag{6}
$$
其中，$P(c)$是类$c$的先验概率，$P(x|c)$是对于类$c$的条件概率，$P(x)$是归一化因子

故问题转化为如何得到$P(c)$和$P(x|c)$



可通过大数定律，根据训练数据集中类别$c$出现的频率进行估计$P(c)=\frac{|D_c|}{|D|}$



### 估计条件概率



#### 极大似然估计(MLE)



条件概率$P(x|c)$的估计是参数估计的过程，统计学界的两个学派分别提供了不同的解决方案：

​	1.**频率主义学派**：认为参数虽然未知，但是为客观存在的固定值

​	2.**贝叶斯学派**：认为参数是未观测到的随机变量，其本身也有分布

这里采用频率主义学派的**极大似然估计**方法
$$
P(D_c|\theta_c)=\mathop{\prod}_{x\in D_c}P(x|\theta_c)
\tag{7}
$$
即寻找能最大化似然$P(D_c|\theta_c)$的参数$\theta_c$

这种方法相对简单，但是估计结果的准确性严重依赖所假设的概率分布形式是否符合潜在的真实数据分布

在现实任务中，往往需要利用关于任务的经验知识来确定



#### 朴素贝叶斯分类器(NB)



估计条件概率的难点在于，$P(x|c)$是联合概率，因为样本数量有限，不能够用频率代替概率

朴素贝叶斯分类器采用了**属性条件独立性假设：**假设所有属性相互对立，即
$$
P(x|c)=\prod^d_{i=1}P(x_i|c)
\tag{8}
$$
其中，$d$为属性数量，则贝叶斯判断准则为
$$
h_{nb}(x)=\mathop{argmax}_{c\in Y}P(c)\prod^d_{i=1}P(x_i|c)
\tag{9}
$$
其中，$P(x_i|c)=\frac{|D_{c,x_i}|}{|D_c|}$，若为连续属性可考虑概率密度函数



若某属性$x_i$在$D_c$中未出现过，则$P(x_i|c)=0$，连乘后仍为0，不合理

通常采用**拉普拉斯修正**：
$$
P(c)=\frac{|D_c|+1}{|D|+N}\\
P(x_i|c)=\frac{|D_{c,x_i|+1}}{|D_c|+N_i}
\tag{10}
$$
其中，$N$为类别的取值数，$N_i$为第$i$个属性的可能取值数



若对任务速度有要求，则可提前将会用到概率值存起来，进行预测时**查表**即可

若任务数据经常变动，则可采取**懒惰学习**的方式，等到需要预测时再计算概率

若任务数据不断增加，则可采取**增量学习**的方式



